# Media Analytics Pipeline

![Pipeline Diagram](https://github.com/MrJohn91/platform_campaign_analysis/blob/main/visualization/diagram.png)

## Overview

This project analyzes a year of social media campaign data from four sources to visualize trends, compare device performance, and provide key insights into Metaâ€™s weekly campaigns and their duration. The solution implements a scalable and automated pipeline that processes campaign performance data, generating visual reports on metrics like impressions, clicks, and views. These reports are accessible via a FastAPI-based web service and are generated within a Docker container, ensuring easy deployment and operation in a production environment.

## Features

- **Campaign Performance Analysis:** Analyze data from four sources across various devices.
- **Meta Weekly Campaign Analysis:** Visualize and compare impressions for campaigns, accounting for duration and partial weeks.
- **Dockerized Pipeline:** The solution runs in a Docker container, ensuring it can run in any production environment without external dependencies.
- **Fast-Api** Provides a web interface to interact with the pipeline and manage the reports
- **Local-Output-storage** Is created and generated reports are saved locally on the user's machine as well as inside the Docker container's output directory.


## Setup and Installation

### 1. Clone the Repository

Clone the repository to your local machine:

```bash
git clone https://github.com/your-username/media-analytics-pipeline.git
cd media-analytics-pipeline
```

### 2. Build the Docker Image

Ensure you have Docker installed on your machine. In the project directory, build the Docker image:

```bash
docker build -t media-analytics .
```

### 3. Run the Docker Container

Run the container with the following command:

```bash
docker run -d \
  -p 8000:8000 \
  -v "$(pwd)/data:/app/data" \
  -v "$(pwd)/output:/app/output" \
  --name analytics \
  media-analytics
```

This command maps the local `data` directory to `/app/data` and the `output` directory to `/app/output` inside the container. The FastAPI application will be available on port 8000.

## Using the API

Once the Docker container is running, you can interact with the pipeline via the Swagger UI.

### 1. Access Swagger UI

Open your browser and go to `http://localhost:8000/docs`. This will display the Swagger UI interface, where you can easily interact with the API.

### 2. Running the Pipeline

In Swagger UI, select the `/run-pipeline` endpoint and click **Try it out**. Then click **Execute** to trigger the pipeline execution.

The pipeline will process the data, generate reports, and store them in the `output` folder.

### 3. Download Generated Reports

Once the pipeline has finished running and the reports are available, you can download them from the API.

In Swagger UI, select the `/download-report/{filename}` endpoint and specify the filename of the report you wish to download. For example:

- `CTR_by_Platform.html`
- `Video_Completions_by_Device_Type_and_Platform.html`
- `total_impressions_by_week.html`

Click **Try it out** and then **Execute** to download the report.

### Available Reports

The following reports are generated by the pipeline:

- `CTR by Platform`
- `Impressions_Over_Time by Platform`
- `Total Impressions by Platform`
- `Video Completion Metrics by_Platform`
- `Video Completions rate by Device_Type and Platform`
- `Meta weekly impressions dataframe`
- `Meta total_impressions by week.`
- `Meta weekly impressions by Campaign durations`

## Directory Structure

- `data/`: Directory for input data files plus clean processed files for analysis.
- `output/`: Directory where generated reports will be saved.
- `api.py`: FastAPI server for handling requests.
- `Dockerfile`: Docker setup for the pipeline and FastAPI application.
- `media_analytics_pipeline.py`: The main code for the analytics pipeline.
- `requirements.txt`: List of Python dependencies.

## Next Steps

To automate the pipeline execution without manual triggering:

- **Automatic Scheduling within server using APScheduler**  
  Schedule the pipeline to run at intervals like daily or weekly by using APScheduler inside the FastAPI server.

- **Cron Job for Heavy Workloads**  
  For larger workloads or production-ready setups, it is recommended to schedule the pipeline externally using a cron job or a cloud-based task scheduler. This offloads the scheduling responsibility from the FastAPI server and provides better control and scalability.


## Troubleshooting

- **Error: Report not found.**  
  Ensure the pipeline is executed first via the `/run-pipeline` endpoint before attempting to download reports.

- **Error: Pipeline execution failed.**  
  Check the logs for any error messages during pipeline execution. You can view the logs using:
  
  ```bash
  docker logs -f analytics
